{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data and Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tom Bliss\\Anaconda3\\lib\\site-packages\\smart_open\\ssh.py:34: UserWarning: paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n",
      "  warnings.warn('paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim import corpora\n",
    "from gensim import models\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "import string\n",
    "from scipy.sparse import hstack\n",
    "from scipy.sparse import vstack\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import ElasticNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('training_set.csv',\n",
    "                       encoding = 'latin-1',\n",
    "                       parse_dates = ['Created'])\n",
    "\n",
    "df_hold = pd.read_csv('holdout_set.csv',\n",
    "                      encoding = 'latin-1',\n",
    "                      parse_dates = ['Created'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_train.drop('Engagements', axis = 1)\n",
    "X['data_type'] = \"training\"\n",
    "df_hold['data_type'] = \"hold\"\n",
    "X = X.append(df_hold.drop('Engagements', axis = 1))\n",
    "\n",
    "Y = df_train['Engagements']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Series - Month Seasonality with Trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#doing this to be able to put this into linear regression\n",
    "X['month'] = X.Created.apply(lambda x: x.month) #seasonal term\n",
    "X['year_month'] = X.Created.apply(lambda x: x.month + x.year * 12) #trend term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Series - Hourly with day of Week and Trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#doing this to be able to put this into linear regression\n",
    "X['hour'] = X.Created.apply(lambda x: x.hour) #seasonal term\n",
    "X['weekend'] = X.Created.apply(lambda x: int(x.dayofweek >= 5)) #seasonal term\n",
    "X['weekend_hour_interaction'] = X.Created.apply(lambda x: int(x.dayofweek >= 5) * x.hour) #seasonal term\n",
    "X['weekday_hour_interaction'] = X.Created.apply(lambda x: int(x.dayofweek < 5) * x.hour) #seasonal term\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding features\n",
    "X['day_of_week'] = X.Created.apply(lambda x: x.dayofweek)\n",
    "X = pd.get_dummies(X, columns = [\"day_of_week\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "###just one hot encoding everything (not commented out)\n",
    "X = pd.get_dummies(X, columns = ['hour', 'weekend', 'weekend_hour_interaction', 'weekday_hour_interaction', 'month'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Features from text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filling NA with empty text\n",
    "X.Description.fillna(\"\", inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#copied from AML, feel free to add your AML stuff\n",
    "\n",
    "X['containsLink'] = X.Description.str.contains('.http').astype(float)\n",
    "X['exclamationPointCount'] =X.Description.str.count('!').astype(float)\n",
    "X['questionMarkCount'] = X.Description.str.count('\\?').astype(float)\n",
    "X['doubleQuotationMarkCount'] = X.Description.str.count('\\\"').astype(float)\n",
    "X['singleQuoteMarkCount'] = X.Description.str.count('\\'').astype(float)\n",
    "X['commaMarkCount'] = X.Description.str.count(',').astype(float)\n",
    "X['collinCount'] = X.Description.str.count(':').astype(float)\n",
    "X['semiCollinCount'] = X.Description.str.count(';').astype(float)\n",
    "X['percentMarkCount'] = X.Description.str.count('%').astype(float)\n",
    "X['dollarSignCount'] = X.Description.str.count('$').astype(float)\n",
    "X['hashCount'] = X.Description.str.count('#').astype(float)\n",
    "X['starCount'] = X.Description.str.count('\\*').astype(float)\n",
    "X['atCount'] = X.Description.str.count('@').astype(float)\n",
    "X['percentCapital'] = (X.Description.str.findall(r'[A-Z]').str.len().fillna(0)/X.Description.str.len().fillna(1)).fillna(0)\n",
    "X['percentlowercase'] = (X.Description.str.findall(r'[a-z]').str.len().fillna(0)/X.Description.str.len().fillna(1)).fillna(0)\n",
    "X['percentnumbers'] = (X.Description.str.findall(r'[0-9]').str.len().fillna(0)/X.Description.str.len().fillna(1)).fillna(0)\n",
    "X['percentother'] = (1 - X['percentCapital'] - X['percentlowercase'] - X['percentnumbers']).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading data from Google\n",
    "\n",
    "#perhaps use one better for twitter data\n",
    "w = models.KeyedVectors.load_word2vec_format(\n",
    "    'GoogleNews-vectors-negative300.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [[token for token in doc.translate(str.maketrans('', '', string.punctuation)).lower().split()]\n",
    "               for doc in (X['Description']).astype(str)]\n",
    "\n",
    "texts_final = []\n",
    "\n",
    "for i in range(len(texts)):\n",
    "    doc_final = []\n",
    "    for j in range(len(texts[i])):\n",
    "            if texts[i][j] in w:\n",
    "                doc_final.append(texts[i][j])    \n",
    "    if len(doc_final) < 1:\n",
    "        texts_final.append(['NA'])\n",
    "    else:\n",
    "        texts_final.append(doc_final)\n",
    "        \n",
    "embedding = np.vstack([np.mean(w[doc], axis=0) for doc in texts_final])\n",
    "\n",
    "for i in range(len(embedding[0])):\n",
    "    X['embedding_' + str(i)] = embedding[:,i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating dummies for type column\n",
    "X = pd.get_dummies(X, columns = [\"Type\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X.loc[X.data_type == \"training\"].drop(\"data_type\", axis = 1),\n",
    "                                                    Y,\n",
    "                                                    random_state = 23)\n",
    "\n",
    "\n",
    "#count vectorizor\n",
    "vect = TfidfVectorizer()\n",
    "X_train_sparse = vect.fit_transform(X_train.Description)\n",
    "X_test_sparse = vect.transform(X_test.Description)\n",
    "\n",
    "X_train.drop(['Description', \"Created\"], axis = 1, inplace = True)\n",
    "X_test.drop(['Description', \"Created\"], axis = 1, inplace = True)\n",
    "\n",
    "\n",
    "for feature in X_train.columns:\n",
    "    X_train_sparse = hstack((X_train_sparse, np.array(X_train[feature]).reshape(-1,1)))\n",
    "    X_test_sparse = hstack((X_test_sparse, np.array(X_test[feature]).reshape(-1,1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting / Testing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression().fit(X_train_sparse, y_train)\n",
    "l = Lasso().fit(X_train_sparse, y_train)\n",
    "r = Ridge().fit(X_train_sparse, y_train)\n",
    "en = ElasticNet().fit(X_train_sparse, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1401443974547446\n",
      "0.06669218817417247\n",
      "0.31705360523026477\n",
      "0.19723597265100756\n"
     ]
    }
   ],
   "source": [
    "MAPE_lr = sum(abs((y_test - lr.predict(X_test_sparse)) / y_test))/len(y_test)\n",
    "MAPE_l = sum(abs((y_test - l.predict(X_test_sparse)) / y_test))/len(y_test)\n",
    "MAPE_r = sum(abs((y_test - r.predict(X_test_sparse)) / y_test))/len(y_test)\n",
    "MAPE_en = sum(abs((y_test - en.predict(X_test_sparse)) / y_test))/len(y_test)\n",
    "print(MAPE_lr)\n",
    "print(MAPE_l)\n",
    "print(MAPE_r)\n",
    "print(MAPE_en)\n",
    "\n",
    "#lasso gives best score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001:\n",
      "0.10662748189101248\n",
      "\n",
      "\n",
      "0.01:\n",
      "0.10463546937509147\n",
      "\n",
      "\n",
      "0.1:\n",
      "0.08964899447555166\n",
      "\n",
      "\n",
      "1:\n",
      "0.06669218817417247\n",
      "\n",
      "\n",
      "10:\n",
      "0.05746811417025286\n",
      "\n",
      "\n",
      "100:\n",
      "0.06245284724795054\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#cant find grid search for MAPE, doing it manually:\n",
    "\n",
    "for vals in [0.001, 0.01, 0.1, 1, 10, 100]:\n",
    "    Lasso_grid = Lasso(alpha = vals).fit(X_train_sparse, y_train)\n",
    "    print(str(vals) + \":\")\n",
    "    print(sum(abs((y_test - Lasso_grid.predict(X_test_sparse)) / y_test))/len(y_test))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting on Holdout Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.051544797524904894\n"
     ]
    }
   ],
   "source": [
    "#refitting tfidf with full data set\n",
    "X_hold = X.loc[X.data_type == \"hold\"].drop(\"data_type\", axis = 1)\n",
    "\n",
    "X_train_total = X.loc[X.data_type == \"training\"].drop(\"data_type\", axis = 1)\n",
    "\n",
    "vect = TfidfVectorizer()\n",
    "X_train_total_sparse = vect.fit_transform(X_train_total.Description)\n",
    "X_hold_sparse = vect.transform(X_hold.Description)\n",
    "\n",
    "X_hold.drop(['Description', \"Created\"], axis = 1, inplace = True)\n",
    "\n",
    "for feature in X_train.columns:\n",
    "    X_train_total_sparse = hstack((X_train_total_sparse, np.array(X_train_total[feature]).reshape(-1,1)))\n",
    "    X_hold_sparse = hstack((X_hold_sparse, np.array(X_hold[feature]).reshape(-1,1)))\n",
    "\n",
    "#refitting lasso on full data set    \n",
    "l = Lasso(alpha = 10).fit(X_train_total_sparse, Y)\n",
    "\n",
    "#double checking this was done correctly\n",
    "print(sum(abs((Y - l.predict(X_train_total_sparse)) / Y))/len(Y))\n",
    "\n",
    "\n",
    "df_hold.drop('data_type', axis = 1, inplace = True)\n",
    "df_hold.Engagements = l.predict(X_hold_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hold.to_csv(\"holdout_set_Columbia.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
